{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from model import HierarchicalAttentionNetwork\n",
    "from dt import News20Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "#from transformers import DistilBertForSequenceClassification, Trainer, TrainingArgument\n",
    "from transformers import BertConfig\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from utils import get_pretrained_weights, Tokenizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the size of training dataset\n",
    "raw_data = fetch_20newsgroups(\n",
    "    data_home='data/news20',\n",
    "    subset='train',\n",
    "    categories=['sci.crypt', 'sci.electronics', 'sci.med', 'sci.space'],\n",
    "    shuffle=False,\n",
    "    remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = raw_data['data'], raw_data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42) \n",
    "maximum_vocab_size = 10000\n",
    "min_count = 3\n",
    "tokenizor = Tokenizor(X, maximum_vocab_size, min_freq=min_count)\n",
    "train_dataset = News20Dataset(\n",
    "    X_train, y_train, tokenizor, 200,40)\n",
    "val_dataset = News20Dataset(\n",
    "    X_test, y_test, tokenizor, 200, 40)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path ='bert-base-uncased',num_labels=4)\n",
    "model.train()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#dev = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "            \n",
    "\n",
    "\n",
    "def get_preprocess(dev):\n",
    "    def preprocess(*batch_data):\n",
    "        return [x.to(torch.int64).to(dev) for x in batch_data]\n",
    "    \n",
    "    return preprocess\n",
    "preprocess = get_preprocess(dev)\n",
    "val_dl = MyDataLoader(val_dataset, batch_size=32)\n",
    "train_dl = MyDataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "val_dl = WrappedDataLoader(val_dl, preprocess)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ybR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss = self.criterion(scores, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            if self.config.max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), self.config.max_grad_norm)\n",
    "\n",
    "            # NOTE MODIFICATION (BUG): clip grad norm should come before optimizer.step()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Compute accuracy\n",
    "            predictions = scores.max(dim=1)[1]\n",
    "            correct_predictions = torch.eq(predictions, labels).sum().item()\n",
    "            acc = correct_predictions\n",
    "\n",
    "            self.losses.update(loss.item(), batch_size)\n",
    "            self.accs.update(acc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=4\n",
    "vocab_size=tokenizor.vocab_size\n",
    "embed_dim=300\n",
    "word_gru_hidden_dim=256\n",
    "sent_gru_hidden_dim=256\n",
    "word_gru_num_layers=1\n",
    "sent_gru_num_layers=1\n",
    "word_att_dim=64\n",
    "sent_att_dim=64\n",
    "use_layer_norm=True\n",
    "dropout=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HierarchicalAttentionNetwork(\n",
    "    num_classes=4,\n",
    "    vocab_size=tokenizor.vocab_size+10,\n",
    "    embed_dim=300,\n",
    "    word_gru_hidden_dim=256,\n",
    "    sent_gru_hidden_dim=256,\n",
    "    word_gru_num_layers=1,\n",
    "    sent_gru_num_layers=1,\n",
    "    word_att_dim=64,\n",
    "    sent_att_dim=64,\n",
    "    use_layer_norm=True,\n",
    "    dropout=0.5).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Fasttext' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6622bd35d173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = Fasttext(output_size= train_dataset.num_classes,\n\u001b[0m\u001b[1;32m      3\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0membedding_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;31m#self, batch_size, output_size, hidden_size, vocab_size,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Fasttext' is not defined"
     ]
    }
   ],
   "source": [
    "#self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "model = Fasttext(output_size= train_dataset.num_classes,\n",
    "            vocab_size=train_dataset.vocab_size,\n",
    "            embedding_length=300,\n",
    "            dropout=0.2#self, batch_size, output_size, hidden_size, vocab_size, \n",
    ")\n",
    "from utils import get_pretrained_weights\n",
    "model.word_embeddings.weights = get_pretrained_weights(\"data/glove\", train_dataset.vocab, 300, dev)\n",
    "model.train()\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 --------------------------------------------\n",
      "iterate 0 loss 1.40\n",
      "iterate 50 loss 1.28\n",
      "precision 0.3455056179775281\n",
      "1 --------------------------------------------\n",
      "iterate 100 loss 1.26\n",
      "precision 0.47331460674157305\n",
      "2 --------------------------------------------\n",
      "iterate 150 loss 0.78\n",
      "precision 0.6643258426966292\n",
      "3 --------------------------------------------\n",
      "iterate 200 loss 0.41\n",
      "precision 0.7331460674157303\n",
      "4 --------------------------------------------\n",
      "iterate 250 loss 0.53\n",
      "precision 0.7205056179775281\n",
      "5 --------------------------------------------\n",
      "iterate 300 loss 0.28\n",
      "precision 0.7528089887640449\n",
      "6 --------------------------------------------\n",
      "iterate 350 loss 0.27\n",
      "precision 0.7682584269662921\n",
      "7 --------------------------------------------\n",
      "iterate 400 loss 0.05\n",
      "precision 0.7570224719101124\n",
      "8 --------------------------------------------\n",
      "iterate 450 loss 0.04\n",
      "precision 0.7780898876404494\n",
      "9 --------------------------------------------\n",
      "iterate 500 loss 0.11\n",
      "precision 0.7724719101123596\n",
      "10 --------------------------------------------\n",
      "iterate 550 loss 0.12\n",
      "precision 0.7556179775280899\n",
      "11 --------------------------------------------\n",
      "iterate 600 loss 0.25\n",
      "precision 0.7176966292134831\n",
      "12 --------------------------------------------\n",
      "iterate 650 loss 0.23\n",
      "precision 0.7289325842696629\n",
      "13 --------------------------------------------\n",
      "iterate 700 loss 0.42\n",
      "precision 0.6966292134831461\n",
      "14 --------------------------------------------\n",
      "iterate 750 loss 0.92\n",
      "precision 0.6179775280898876\n"
     ]
    }
   ],
   "source": [
    "max_grad_norm = None\n",
    "for epoch in range(15):\n",
    "    print(epoch,'--------------------------------------------')\n",
    "    for i, (docs, labels, doc_lengths, sent_lengths) in enumerate(train_dl):\n",
    "        model.train()\n",
    "        scores, word_att_weights, sentence_att_weights = model(\n",
    "                docs, doc_lengths, sent_lengths)\n",
    "        loss = F.cross_entropy(scores, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        if max_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.model.parameters(), max_grad_norm)\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i + len(train_dl) * epoch) % 50==0:\n",
    "            print('iterate %d loss %.2f' %(i + len(train_dl) * epoch, loss))\n",
    "   \n",
    "    model_eval(model, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "precision 0.7542134831460674\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# the size of training dataset\n",
    "raw_data = fetch_20newsgroups(\n",
    "    data_home='data/news20',\n",
    "    subset='test',\n",
    "    categories=['sci.crypt', 'sci.electronics', 'sci.med', 'sci.space'],\n",
    "    shuffle=False,\n",
    "    remove=('headers', 'footers', 'quotes'))\n",
    "X, y = raw_data['data'], raw_data['target']\n",
    "test_dataset = News20Dataset(X_test, y_test, \"data/glove/glove.6B.300d.txt\", 200) \n",
    "\n",
    "test_dl = DataLoader(test_dataset, batch_size=32)\n",
    "test_dl = WrappedDataLoader(test_dl, preprocess)\n",
    "\n",
    "model_eval(model, test_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('speller': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f004e67df22de638fc0b182653e9eee592cf49ac1e8763b3fa1542a56699d278"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}